# ğŸ¤– AI/ML Paper Implementations

AI/ML í•µì‹¬ ê°œë…ê³¼ ë…¼ë¬¸ êµ¬í˜„ì„ ìœ„í•œ í•™ìŠµ ì €ì¥ì†Œì…ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [BERT & Attention ë°ëª¨](#bert--attention-ë°ëª¨)

---

## BERT & Attention ë°ëª¨

### ğŸ“„ íŒŒì¼: `bert_attention_demo.py`

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‘ ê°€ì§€ í•µì‹¬ NLP ê°œë…ì„ ì‹œì—°í•©ë‹ˆë‹¤:

### 1ï¸âƒ£ BERT Tokenization (Pre-training)

BERTëŠ” ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê° í† í°ì— ê³ ìœ  IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "Machine learning is powerful."
inputs = tokenizer(text, return_tensors="pt")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ì…ë ¥ ë¬¸ì¥: Machine learning is powerful.
í† í° ID: tensor([[ 101, 3698, 4083, 2003, 3928, 1012, 102]])
```

| í† í° ID | ì˜ë¯¸ |
|---------|------|
| 101 | [CLS] - ë¬¸ì¥ ì‹œì‘ |
| 3698 | "machine" |
| 4083 | "learning" |
| 2003 | "is" |
| 3928 | "powerful" |
| 1012 | "." |
| 102 | [SEP] - ë¬¸ì¥ ë |

### 2ï¸âƒ£ Simple Attention (Attention Is All You Need)

"Attention Is All You Need" ë…¼ë¬¸ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¨ìˆœí™”í•œ êµ¬í˜„ì…ë‹ˆë‹¤.

```python
def simple_attention(q, k, v):
    scores = torch.matmul(q, k.transpose(-2, -1))
    weights = torch.nn.functional.softmax(scores, dim=-1)
    return torch.matmul(weights, v), weights
```

**Attention ìˆ˜ì‹:**
```
Attention(Q, K, V) = softmax(Q Ã— K^T) Ã— V
```

**ì¶œë ¥ ì˜ˆì‹œ (3ê°œ ë‹¨ì–´ ê°„ ì§‘ì¤‘ë„):**
```
[[[0.9999  0.0000  0.0001]
  [0.0000  1.0000  0.0000]
  [0.0000  0.0000  1.0000]]]
```

ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ "ì§‘ì¤‘(attention)"í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì…ë‹ˆë‹¤.

---

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install torch transformers
```

### ì‹¤í–‰
```bash
python bert_attention_demo.py
```

---

## ğŸ“– ì°¸ê³  ë…¼ë¬¸

1. **BERT**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (Devlin et al., 2018)
2. **Transformer**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)

---

## ğŸ“… ì—…ë°ì´íŠ¸ ë¡œê·¸

- **2026-01-19**: BERT Tokenization & Simple Attention ë°ëª¨ ì¶”ê°€

---

## ğŸ“œ License

MIT License
