# ğŸ¤– AI/ML Paper Implementations

AI/ML í•µì‹¬ ê°œë…ê³¼ ë…¼ë¬¸ êµ¬í˜„ì„ ìœ„í•œ í•™ìŠµ ì €ì¥ì†Œì…ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [BERT & Attention ë°ëª¨](#bert--attention-ë°ëª¨)
2. [ML Ensemble & SHAP ë°ëª¨](#ml-ensemble--shap-ë°ëª¨)

---

## BERT & Attention ë°ëª¨

### ğŸ“„ íŒŒì¼: `bert_attention_demo.py`

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‘ ê°€ì§€ í•µì‹¬ NLP ê°œë…ì„ ì‹œì—°í•©ë‹ˆë‹¤:

### 1ï¸âƒ£ BERT Tokenization (Pre-training)

BERTëŠ” ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê° í† í°ì— ê³ ìœ  IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "Machine learning is powerful."
inputs = tokenizer(text, return_tensors="pt")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ì…ë ¥ ë¬¸ì¥: Machine learning is powerful.
í† í° ID: tensor([[ 101, 3698, 4083, 2003, 3928, 1012, 102]])
```

| í† í° ID | ì˜ë¯¸ |
|---------|------|
| 101 | [CLS] - ë¬¸ì¥ ì‹œì‘ |
| 3698 | "machine" |
| 4083 | "learning" |
| 2003 | "is" |
| 3928 | "powerful" |
| 1012 | "." |
| 102 | [SEP] - ë¬¸ì¥ ë |

### 2ï¸âƒ£ Simple Attention (Attention Is All You Need)

"Attention Is All You Need" ë…¼ë¬¸ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¨ìˆœí™”í•œ êµ¬í˜„ì…ë‹ˆë‹¤.

```python
def simple_attention(q, k, v):
    scores = torch.matmul(q, k.transpose(-2, -1))
    weights = torch.nn.functional.softmax(scores, dim=-1)
    return torch.matmul(weights, v), weights
```

**Attention ìˆ˜ì‹:**
```
Attention(Q, K, V) = softmax(Q Ã— K^T) Ã— V
```

**ì¶œë ¥ ì˜ˆì‹œ (3ê°œ ë‹¨ì–´ ê°„ ì§‘ì¤‘ë„):**
```
[[[0.9999  0.0000  0.0001]
  [0.0000  1.0000  0.0000]
  [0.0000  0.0000  1.0000]]]
```

ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ "ì§‘ì¤‘(attention)"í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì…ë‹ˆë‹¤.

---

## ML Ensemble & SHAP ë°ëª¨

### ğŸ“„ íŒŒì¼: `ml_ensemble_shap_demo.py`

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì•™ìƒë¸” ê¸°ë²•ê³¼ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±(XAI)ì„ ì‹œì—°í•©ë‹ˆë‹¤.

### 1ï¸âƒ£ Random Forest (Ensemble Learning)

ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ì¡°í•©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì…ë‹ˆë‹¤.

```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
```

**Feature Importance ê²°ê³¼:**
```
petal length (cm): 0.4400  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
petal width (cm):  0.4215  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
sepal length (cm): 0.1081  â–ˆâ–ˆâ–ˆâ–ˆ
sepal width (cm):  0.0304  â–ˆ
```

### 2ï¸âƒ£ XGBoost (Gradient Boosting)

ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë¶€ìŠ¤íŒ… ê¸°ë²•ì…ë‹ˆë‹¤.

```python
import xgboost as xgb

xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)
xgb_model.fit(X_train, y_train)
```

### 3ï¸âƒ£ SHAP (SHapley Additive exPlanations)

ê° íŠ¹ì§•ì´ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ ì„¤ëª…í•˜ëŠ” XAI ê¸°ë²•ì…ë‹ˆë‹¤.

```python
import shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)
```

**SHAP Feature Importance:**
```
petal length (cm): 2.0761  #########################################
petal width (cm):  0.5093  ##########
sepal length (cm): 0.1249  ##
sepal width (cm):  0.1213  ##
```

### ğŸ“Š SHAP Summary Plot

![SHAP Summary Plot](shap_summary_plot.png)

**ëª¨ë¸ ì„±ëŠ¥:**
| ëª¨ë¸ | ì •í™•ë„ |
|------|--------|
| Random Forest | 100.0% |
| XGBoost | 100.0% |

---

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# BERT & Attention ë°ëª¨
pip install torch transformers

# ML Ensemble & SHAP ë°ëª¨
pip install numpy pandas scikit-learn xgboost shap matplotlib
```

### ì‹¤í–‰
```bash
# BERT Tokenization & Attention
python bert_attention_demo.py

# ML Ensemble & SHAP
python ml_ensemble_shap_demo.py
```

---

## ğŸ“– ì°¸ê³  ë…¼ë¬¸

1. **BERT**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (Devlin et al., 2018)
2. **Transformer**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
3. **Random Forest**: [Random Forests](https://link.springer.com/article/10.1023/A:1010933404324) (Breiman, 2001)
4. **XGBoost**: [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754) (Chen & Guestrin, 2016)
5. **SHAP**: [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874) (Lundberg & Lee, 2017)

---

## ğŸ“… ì—…ë°ì´íŠ¸ ë¡œê·¸

- **2026-01-19**: ML Ensemble & SHAP ë°ëª¨ ì¶”ê°€
- **2026-01-19**: BERT Tokenization & Simple Attention ë°ëª¨ ì¶”ê°€

---

## ğŸ“œ License

MIT License
